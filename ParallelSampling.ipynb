{"cells":[{"cell_type":"code","source":["#### INSTALLING REQUIRED LIBRARIES ####\n%pip install fastavro\n%pip install multipledispatch\n#######################################"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4f8eae29-24f6-432c-9b8f-043b5d5df3e4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Python interpreter will be restarted.\nRequirement already satisfied: fastavro in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e6cad200-a3c3-4ded-bc68-960fe0805e3a/lib/python3.9/site-packages (1.6.0)\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nRequirement already satisfied: multipledispatch in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e6cad200-a3c3-4ded-bc68-960fe0805e3a/lib/python3.9/site-packages (0.6.0)\nRequirement already satisfied: six in /databricks/python3/lib/python3.9/site-packages (from multipledispatch) (1.16.0)\nPython interpreter will be restarted.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Python interpreter will be restarted.\nRequirement already satisfied: fastavro in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e6cad200-a3c3-4ded-bc68-960fe0805e3a/lib/python3.9/site-packages (1.6.0)\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nRequirement already satisfied: multipledispatch in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e6cad200-a3c3-4ded-bc68-960fe0805e3a/lib/python3.9/site-packages (0.6.0)\nRequirement already satisfied: six in /databricks/python3/lib/python3.9/site-packages (from multipledispatch) (1.16.0)\nPython interpreter will be restarted.\n"]}}],"execution_count":0},{"cell_type":"code","source":["####################CONFIGURATION###################################\nDATA_PATH = '/dbfs/FileStore/shared_uploads/prem.potta@shipt.com/orders_data.json'\nINPUT_SCHEMA_PATH = '/dbfs/FileStore/shared_uploads/prem.potta@shipt.com/orders_data_schema.avsc'\nOUTPUT_SCHEMA_PATH = '/dbfs/FileStore/shared_uploads/prem.potta@shipt.com/output_schema.avsc'\nNO_OF_WORKERS = 10\n###################################################################"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"66097ea1-3bdc-4579-ade2-ffa9dd696163"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession, SQLContext\nfrom pyspark.sql.functions import col, spark_partition_id, asc, desc\nimport pyspark.sql.functions as F \nfrom pyspark.sql.types import StringType, StructType, StructField, DoubleType\nfrom pyspark.conf import SparkConf\nfrom pyspark.context import SparkContext"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f606428-9eed-45f4-8955-16fbaebb8613"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Configuring spark cluster\nconf = SparkConf()\nconf.set(\"spark.kryo.registrationRequired\",\"false\")\nconf.setMaster(\"local\").setAppName(\"testapp\")\n\n# Gets an existing SparkSession or, if there is no existing one, creates a new one based on the options set in the builder\nsc = SparkContext.getOrCreate(conf=conf)\nspark=SparkSession.builder.appName(\"test\").config(\n    \"spark.driver.extraJavaOptions\",\n    \"--add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED\",\n).getOrCreate()\n\n# Creating Spark SQL Context\nsqlContext = SQLContext(sc)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5aad6ba6-8b66-4c31-9f78-b910393f9853"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"/databricks/spark/python/pyspark/sql/context.py:117: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["/databricks/spark/python/pyspark/sql/context.py:117: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\n"]}}],"execution_count":0},{"cell_type":"code","source":["import json\nfrom fastavro.validation import validate\nfrom fastavro.schema import parse_schema,load_schema\nimport pandas as pd\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.functions import col, spark_partition_id, asc, desc\nimport pyspark.sql.functions as F \nfrom pyspark.sql.types import StringType, StructType, StructField, DoubleType"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d195dbcb-6566-4dbe-a0c6-3d9ddaea9162"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Load and parse Avro Schema from its file path\nparsed_schema = parse_schema(load_schema(INPUT_SCHEMA_PATH))\n\n# Open the data file and validate it with the corresponding Avro Schema\nwith open(DATA_PATH) as json_file:\n    data = json.load(json_file)\n    if validate(data, parsed_schema):\n        print(\"Successfully validated the data with its schema\" )\n    else:\n        raise Exception(\"Failed to validate the data with its schema\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7fe2ad37-179c-4921-8c22-f99d2b9e6ce5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Successfully validated the data with its schema\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Successfully validated the data with its schema\n"]}}],"execution_count":0},{"cell_type":"code","source":["import abc\nfrom multipledispatch import dispatch\nfrom itertools import chain\nimport random\n\n# Creating an abstract base class\nclass AbstractJob(metaclass=abc.ABCMeta):\n    \n    # Initialize the job with its input data, input schema and output schema\n    def __init__(self, input_json, input_schema, output_schema):\n        self.input_json=input_json\n        self.input_schema=input_schema\n        self.output_schema=output_schema\n        self.validateInputData()\n    \n    # Return the input data for a job\n    @property\n    def inputData(self):\n        return self.input_json\n    \n    # Return the input data schema for a job\n    @property\n    def inputDataSchema(self):\n        return self.input_schema\n\n    # Validate input data against it's avro schema\n    def validateInputData(self):\n        if not validate(json.loads(self.input_json), json.loads(self.input_schema)):\n            raise Exception(\"Input data validation failed\")\n        else:\n            print('Input Data Validation Successful')\n    \n    # Return the output schema for a job\n    @property\n    def outputDataSchema(self):\n        return self.output_schema\n\n    # Run this function to validate the output data obtained after sampling\n    def validateOutputData(self,output_data):\n        if not validate(output_data, json.loads(self.output_schema)):\n            raise Exception(\"Output data validation failed\")\n        else:\n            print('Output Data Validation Successful')\n            \n    # Abstract method that can be overrided for different kinds of parallel jobs\n    @staticmethod\n    @dispatch(object)\n    @abc.abstractmethod\n    def process(job):\n        # translate json to avro objects in java/c++/python\n        pass\n\n    # Abstract method that takes input data, input schema and output schema for each worker\n    @staticmethod\n    @dispatch(str,str,str)\n    @abc.abstractmethod\n    def process(input_json, input_schema, output_schema):\n        # translate json to avro objects in java/c++/python\n        pass\n    \n    # Abstract method to define the parallel implementation of each worker\n    @staticmethod\n    @abc.abstractmethod\n    def parallelProcess(input_json, input_schema, output_schema):\n        pass\n\n    # Implementation of distributed filtering based random sampling algorithm\n    @staticmethod\n    def runParallel(jobs):\n        \n        # Get the process function for each job\n        process = jobs[0].parallelProcess\n        \n        # Read the input data from the json as a dictionary\n        inputData = jobs[0].inputData\n        inputDict = json.loads(inputData)\n        \n        # Get the list of records that need to be sampled from the json\n        inputList = inputDict['data']\n        \n        # Read the sampling parameters defined in the input json\n        samplingType = inputDict['samplingType']\n        samplingRate = inputDict['samplingRate']\n        print(\"Sampling Rate : \" + str(samplingRate))\n        \n        # Create a spark dataframe from the input data\n        inputDataFrame=spark.createDataFrame(inputList)\n        \n        # Register a temporary table for the data in spark's SQL context\n        inputDataFrame.registerTempTable(\"data\")\n        \n        # Get the filtering query from the input json\n        inputFilter = inputDict['filter']\n        \n        # Run the sparkSQL query on the data frame and map the resulting rdd to a python list of dictionaries\n        results = sqlContext.sql(inputFilter)\n        filteredInputList = results.rdd.map(lambda row: row.asDict()).collect()\n        print('Filtered ' + str(len(filteredInputList)) + ' out of ' + str(len(inputList)) + ' records')\n        \n        # Split the list of records to multiple lists that can be processed by each worker\n        n = len(jobs)\n        splitList= [filteredInputList[i:i+n] for i in range(0, len(filteredInputList), n)]\n        \n        # Convert the list to an RDD to perform parallel processing\n        rdd = sc.parallelize(splitList)\n        \n        # Run the process on each worker parallelly using map function of RDD\n        sampledRdd = rdd.map(lambda x : process(x,samplingType,samplingRate)).collect()\n        \n        # Combine output of each worker into a single list\n        output = list(chain.from_iterable(sampledRdd))\n        print(str(len(output)) + ' records out of ' + str(len(filteredInputList)) + ' filtered records have been sampled')\n        \n        # Validate the output data obtained against the output schema defined for the job\n        jobs[0].validateOutputData(output)\n        \n        # Return the output after validation\n        return output"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5f62bc05-a110-4de4-8d20-0632cc4148a3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["class ParallelJob(AbstractJob):\n    \n    # Perform sampling from the input list for different sampling parameters\n    @staticmethod\n    def parallelProcess(inputList,samplingType,samplingRate):\n        if samplingType == \"random\":\n            return random.sample(inputList,int(samplingRate*len(inputList)))\n        return None\n    \n    # Call the appropriate process function which overrides this function\n    @staticmethod\n    @dispatch(object)\n    def process(job):\n        return ParallelJob.process(job.input_json, job.input_schema, job.output_schema)\n    \n    # Run the logic defined in parallelProcess function for each worker\n    @staticmethod\n    @dispatch(str,str,str)\n    def process(input_json, input_schema, output_schema):\n        inputDict = json.loads(input_json)\n        return ParallelJob.parallelProcess(inputDict['data'],inputDict['samplingType'],inputDict['samplingRate'])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e0fa4fe-ec65-427f-bfcd-6e7527709e16"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Create Parallel jobs\njobs=[]\nfor i in range(NO_OF_WORKERS):\n    a=ParallelJob(open(DATA_PATH).read(), open(INPUT_SCHEMA_PATH).read(), open(OUTPUT_SCHEMA_PATH).read())\n    jobs.append(a)"],"metadata":{"scrolled":true,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"79f06de2-43cf-4f75-9938-3a8e846eec35"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Input Data Validation Successful\nInput Data Validation Successful\nInput Data Validation Successful\nInput Data Validation Successful\nInput Data Validation Successful\nInput Data Validation Successful\nInput Data Validation Successful\nInput Data Validation Successful\nInput Data Validation Successful\nInput Data Validation Successful\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Input Data Validation Successful\nInput Data Validation Successful\nInput Data Validation Successful\nInput Data Validation Successful\nInput Data Validation Successful\nInput Data Validation Successful\nInput Data Validation Successful\nInput Data Validation Successful\nInput Data Validation Successful\nInput Data Validation Successful\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Run the parallel jobs and measure the time taken\nimport time\nstart = time.process_time()\noutput=ParallelJob.runParallel(jobs)\nprint(\"Time taken for sampling : \" + str(time.process_time() - start) + \" seconds\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d6d7198f-ab03-4dca-b18c-d036b4aa6ed0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Filtered 93320 out of 191759 records\n9332 records out of 93320 filtered records have been sampled\nSampling Rate : 0.1\nOutput Data Validation Successful\nTime taken for sampling : 23.826951632000032 seconds\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Filtered 93320 out of 191759 records\n9332 records out of 93320 filtered records have been sampled\nSampling Rate : 0.1\nOutput Data Validation Successful\nTime taken for sampling : 23.826951632000032 seconds\n"]}}],"execution_count":0}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.9.7","nbconvert_exporter":"python","file_extension":".py"},"application/vnd.databricks.v1+notebook":{"notebookName":"ParallelSampling","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2474658814746581}},"nbformat":4,"nbformat_minor":0}
